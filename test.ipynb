{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "936e864b",
   "metadata": {},
   "source": [
    "# Comparative Performance Analysis of Pandas, NumPy, and Polars\n",
    "\n",
    "This notebook contains performance, memory usage, and CPU utilization tests comparing three popular Python data processing libraries: **Pandas**, **Numpy**, and **Polars**.\n",
    "\n",
    "The goal is to understand which library is more efficient in handling large datasets and common data operations.\n",
    "\n",
    "*Test environment:* \n",
    "- **OS**: Windows 11 \n",
    "- **CPU**: AMD Ryzen 7 5800 H With Radeon Graphics\n",
    "- **RAM**: 16 GB\n",
    "- **Python Version**: 3.13.5\n",
    "- **Dedicated GPU**: NVIDIA GeForce RTX 3050\n",
    "\n",
    "---\n",
    "\n",
    "*Author:* Sujal Rijal  \n",
    "*Date:* July 2025\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1177e85d",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Below are the versions of libraries and Python used for testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24f9db9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.13.5 (tags/v3.13.5:6cb20a2, Jun 11 2025, 16:15:46) [MSC v.1943 64 bit (AMD64)]\n",
      "Pandas version: 2.3.0\n",
      "NumPy version: 2.3.1\n",
      "Polars version: 1.31.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Polars version: {pl.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ad053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data\n",
    "\n",
    "\n",
    "# large data\n",
    "def generate_test_data(rows=1_000_000):\n",
    "    np.random.seed(42)\n",
    "    data = {\n",
    "        'id': np.arange(rows),\n",
    "        'category': np.random.choice(['A', 'B', 'C', 'D'], rows),\n",
    "        'value': np.random.randn(rows),\n",
    "        'flag': np.random.choice([True, False], rows)\n",
    "    }\n",
    "    return data\n",
    "\n",
    "data = generate_test_data()\n",
    "\n",
    "\n",
    "# Small sample data\n",
    "data_example = {\n",
    "    'category': ['A', 'B', 'A', 'C', 'B', 'C', 'A'],\n",
    "    'value': [10, -5, 20, 0, 15, -3, 8]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e3c6ca",
   "metadata": {},
   "source": [
    "## **EXECUTION TIME**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dcf771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Small Data)\n",
      "Pandas test took: 0.0088 seconds\n",
      "NumPy test took: 0.0005 seconds\n",
      "Polars test took: 0.0052 seconds\n",
      "\n",
      "(Large Data)\n",
      "Pandas test took: 0.8853 seconds\n",
      "NumPy test took: 0.3162 seconds\n",
      "Polars test took: 0.5737 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "def pandas_test(data):\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    filtered = df[df['value'] > 0]\n",
    "\n",
    "    grouped = df.groupby('category')['value'].agg(['sum', 'mean', 'count'])\n",
    "\n",
    "    sorted_df = df.sort_values(['category', 'value'])\n",
    "    return filtered, grouped, sorted_df\n",
    "\n",
    "\n",
    "def polars_test(data):\n",
    "    df = pl.DataFrame(data)\n",
    "    filtered = df.filter(pl.col('value') > 0)\n",
    "    grouped = df.group_by('category').agg([\n",
    "        pl.col('value').sum().alias('sum'),\n",
    "        pl.col('value').mean().alias('mean'),\n",
    "        pl.col('value').count().alias('count')\n",
    "    ])\n",
    "    sorted_df = df.sort(['category', 'value'])\n",
    "    return filtered, grouped, sorted_df\n",
    "\n",
    "\n",
    "def numpy_test(data):\n",
    "    values = np.array(data['value'])\n",
    "    categories = np.array(data['category'])\n",
    "\n",
    "    filtered = values[values > 0]\n",
    "    unique_cats = np.unique(categories)\n",
    "    sums = {}\n",
    "    means = {}\n",
    "    counts = {}\n",
    "    for cat in unique_cats:\n",
    "        cat_mask = categories == cat\n",
    "        sums[cat] = values[cat_mask].sum()\n",
    "        means[cat] = values[cat_mask].mean()\n",
    "        counts[cat] = np.sum(cat_mask)\n",
    "    return filtered, sums, means, counts\n",
    "\n",
    "print(\"(Small Data)\")\n",
    "\n",
    "start = time.time()\n",
    "pandas_res = pandas_test(data_example)\n",
    "end = time.time()\n",
    "print(f\"Pandas test took: {end - start:.4f} seconds\")\n",
    "\n",
    "start = time.time()\n",
    "numpy_res = numpy_test(data_example)\n",
    "end = time.time()\n",
    "print(f\"NumPy test took: {end - start:.4f} seconds\")\n",
    "\n",
    "start = time.time()\n",
    "polars_res = polars_test(data_example)\n",
    "end = time.time()\n",
    "print(f\"Polars test took: {end - start:.4f} seconds\")\n",
    "\n",
    "print(\"\\n(Large Data)\")\n",
    "start = time.time()\n",
    "pandas_res = pandas_test(data)\n",
    "end = time.time()\n",
    "print(f\"Pandas test took: {end - start:.4f} seconds\")\n",
    "\n",
    "start = time.time()\n",
    "numpy_res = numpy_test(data)\n",
    "end = time.time()\n",
    "print(f\"NumPy test took: {end - start:.4f} seconds\")\n",
    "\n",
    "start = time.time()\n",
    "polars_res = polars_test(data)\n",
    "end = time.time()\n",
    "print(f\"Polars test took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da3bf3b",
   "metadata": {},
   "source": [
    "## Analysis Of Result\n",
    "\n",
    "---\n",
    "\n",
    "### **Small Data (8 rows):**\n",
    "- **Numpy: 0.0005 seconds** – Fastest \n",
    "- **Polars: 0.0052 seconds** – Middle \n",
    "- **Pandas: 0.0088 seconds** – Slowest \n",
    "\n",
    "---\n",
    "\n",
    "### **Large Data (100,000 rows):**\n",
    "- **Numpy: 0.3162 seconds** – Still the fastest! \n",
    "- **Polars: 0.5737 seconds** – Good performance with DataFrame functionality \n",
    "- **Pandas: 0.8853 seconds** – Slowest again, but still performs reasonably well\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Observations:**\n",
    "\n",
    "1. **Numpy consistently wins on speed**\n",
    "   - Pure numerical operations on arrays\n",
    "   - No DataFrame overhead\n",
    "   - Backed by highly optimized C implementations\n",
    "\n",
    "2. **Polars vs Pandas performance gap**\n",
    "   - Polars is ~35% faster than Pandas on large datasets\n",
    "   - This is due to Polars' efficient, multi-threaded design\n",
    "\n",
    "3. **Scaling behavior**\n",
    "   - **Numpy**: ~632x slower (0.0005 → 0.3162)\n",
    "   - **Polars**: ~110x slower (0.0052 → 0.5737)\n",
    "   - **Pandas**: ~100x slower (0.0088 → 0.8853)\n",
    "\n",
    "4. **Performance matches CPU usage trends**\n",
    "   - Numpy: Fastest and lowest CPU usage\n",
    "   - Polars: Higher CPU usage but great speed\n",
    "   - Pandas: Slower, less optimized CPU use\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion:\n",
    "For pure numerical tasks, **Numpy is unbeatable**. But if you're working with DataFrames, **Polars** offers a major performance improvement over **Pandas** — making it a strong modern alternative.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2842c26e",
   "metadata": {},
   "source": [
    "## **Memory Usage Check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f80d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Small Data)\n",
      "Memory used by Pandas: 0.0000 MiB\n",
      "Memory used by Polars: 0.0000 MiB\n",
      "Memory used by NumPy: 0.0039 MiB\n",
      "\n",
      "(Large Data)\n",
      "Memory used by Pandas: 97.0703 MiB\n",
      "Memory used by Polars: 77.3633 MiB\n",
      "Memory used by NumPy: 19.0781 MiB\n"
     ]
    }
   ],
   "source": [
    "from memory_profiler import memory_usage\n",
    "\n",
    "def test_memory(func, data, lib_name):\n",
    "    \n",
    "    mem_usage = memory_usage((func, (data,)), interval=0.1)\n",
    "    baseline = mem_usage[0]  \n",
    "    peak = max(mem_usage)    \n",
    "    memory_used = peak - baseline\n",
    "    print(f\"Memory used by {lib_name}: {memory_used:.4f} MiB\")\n",
    "\n",
    "print(\"(Small Data)\")\n",
    "test_memory(pandas_test, data_example, \"Pandas\")\n",
    "test_memory(polars_test, data_example, \"Polars\")\n",
    "test_memory(numpy_test, data_example, \"NumPy\")\n",
    "\n",
    "print(\"\\n(Large Data)\")\n",
    "test_memory(pandas_test, data, \"Pandas\")\n",
    "test_memory(polars_test, data, \"Polars\")\n",
    "test_memory(numpy_test, data, \"NumPy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c66917e",
   "metadata": {},
   "source": [
    "I was honestly shocked by these results! Especially since I haven't used polars before, but now i am really eager to dive into it and explore its capabilities.\n",
    "\n",
    "**Small Data (8 rows):**\n",
    "- **Pandas & Polars: 0.0000 MiB** - The dataset is so tiny that any memory allocation is below the measurement threshold (0.1 second intervals might miss quick allocations)\n",
    "- **Numpy: 0.0039 MiB** - Shows a small measurable increase, likely from array conversions and dictionary operations\n",
    "\n",
    "**Large Data (100,000 rows):**\n",
    "- **Pandas: 97.0703 MiB** - Highest memory usage, which is expected because:\n",
    "  - Creates multiple DataFrame copies during operations (filtering, grouping, sorting)\n",
    "  - Less memory-efficient for large datasets\n",
    "  - Intermediate objects during chained operations\n",
    "\n",
    "- **Polars: 77.3633 MiB** - Middle ground, which makes sense because:\n",
    "  - More memory-efficient than Pandas\n",
    "  - Uses lazy evaluation and optimized memory management\n",
    "  - Better at avoiding unnecessary copies\n",
    "\n",
    "- **Numpy: 19.0781 MiB** - Lowest memory usage, which is correct because:\n",
    "  - Works directly with arrays (more memory-efficient than DataFrames)\n",
    "  - No overhead from DataFrame structures\n",
    "  - Simple dictionary storage for grouped results\n",
    "\n",
    "So the memory usage ranking here is Numpy < Polars < Pandas m exactly what i’d expect, but seeing Polars perform so efficiently really blew me away.It shows why Numpy is great for pure numerical tasks, but Polars is quickly becoming the exciting new alternative to Pandas for larger, more complex data handling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef638639",
   "metadata": {},
   "source": [
    "## **CPU Usage Check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706334c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Small Data)\n",
      "CPU usage by Pandas: Avg 22.60%, Max 33.00% during 0.0055 seconds\n",
      "CPU usage by Polars: Avg 28.00%, Max 51.60% during 0.0019 seconds\n",
      "CPU usage by NumPy: Avg 12.80%, Max 12.80% during 0.0006 seconds\n",
      "\n",
      "(Large Data)\n",
      "CPU usage by Pandas: Avg 19.93%, Max 45.50% during 1.0512 seconds\n",
      "CPU usage by Polars: Avg 29.18%, Max 69.80% during 0.5457 seconds\n",
      "CPU usage by NumPy: Avg 13.44%, Max 16.50% during 0.3126 seconds\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import time\n",
    "import threading\n",
    "\n",
    "def test_cpu(func, data, lib_name):\n",
    "    cpu_percentages = []\n",
    "    monitoring = [True]\n",
    "    \n",
    "    def monitor_cpu():\n",
    "        \n",
    "        psutil.cpu_percent(interval=None)  \n",
    "        while monitoring[0]:\n",
    "            cpu = psutil.cpu_percent(interval=0.1)\n",
    "            if cpu > 0:  \n",
    "                cpu_percentages.append(cpu)\n",
    "    \n",
    "\n",
    "    monitor_thread = threading.Thread(target=monitor_cpu)\n",
    "    monitor_thread.daemon = True  \n",
    "    monitor_thread.start()\n",
    "    \n",
    "    \n",
    "    time.sleep(0.1)\n",
    "    \n",
    "    \n",
    "    start_time = time.time()\n",
    "    func(data)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    \n",
    "    monitoring[0] = False\n",
    "    monitor_thread.join(timeout=1)  \n",
    "    \n",
    "    \n",
    "    if cpu_percentages:\n",
    "        avg_cpu = sum(cpu_percentages) / len(cpu_percentages)\n",
    "        max_cpu = max(cpu_percentages)\n",
    "    else:\n",
    "        avg_cpu = 0\n",
    "        max_cpu = 0\n",
    "    \n",
    "    elapsed = end_time - start_time\n",
    "    \n",
    "    print(f\"CPU usage by {lib_name}: Avg {avg_cpu:.2f}%, Max {max_cpu:.2f}% during {elapsed:.4f} seconds\")\n",
    "\n",
    "# Example usage:\n",
    "print(\"(Small Data)\")\n",
    "test_cpu(pandas_test, data_example, \"Pandas\")\n",
    "test_cpu(polars_test, data_example, \"Polars\")\n",
    "test_cpu(numpy_test, data_example, \"NumPy\")\n",
    "\n",
    "print(\"\\n(Large Data)\")\n",
    "test_cpu(pandas_test, data, \"Pandas\")\n",
    "test_cpu(polars_test, data, \"Polars\")\n",
    "test_cpu(numpy_test, data, \"NumPy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955337ea",
   "metadata": {},
   "source": [
    "## Result Analysis\n",
    "\n",
    "**Small Data Results:**\n",
    "- **Pandas**: Moderate CPU usage (22.60% avg, 33% max) with relatively slow execution (0.0055s)\n",
    "- **Polars**: Higher CPU usage (28% avg, 51.60% max) but fastest execution (0.0019s) \n",
    "- **Numpy**: Lowest CPU usage (12.80% avg/max) and very fast (0.0006s)\n",
    "\n",
    "**Large Data Results:**\n",
    "- **Pandas**: Moderate CPU usage (19.93% avg, 45.50% max) but slowest execution (1.0512s)\n",
    "- **Polars**: Highest CPU usage (29.18% avg, 69.80% max) but fastest execution (0.5457s)\n",
    "- **Numpy**: Lowest CPU usage (13.44% avg, 16.50% max) with good speed (0.3126s)\n",
    "\n",
    "**Explanation**\n",
    "\n",
    "1. **Polars shows high CPU usage** \n",
    "   - It's aggressively optimized and uses multi-threading\n",
    "   - Higher CPU utilization often means better performance (doing more work per unit time)\n",
    "   - The high max CPU (69.80%) suggests it's effectively using available cores\n",
    "\n",
    "2. **NumPy has low CPU usage** \n",
    "   - It's doing simpler operations (arrays vs DataFrames)\n",
    "   - Less overhead from complex data structures\n",
    "   - More efficient memory access patterns\n",
    "\n",
    "3. **Pandas is in the middle** \n",
    "   - It's single-threaded for most operations\n",
    "   - Has more overhead than NumPy but less optimization than Polars\n",
    "\n",
    "The key insight: **Polars trades higher CPU usage for significantly better execution time**, which is exactly what we want in a performance library. The results demonstrate that Polars is effectively utilizing system resources to deliver faster results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a49c065",
   "metadata": {},
   "source": [
    "### **My Experience Using Pandas and Numpy**\n",
    "\n",
    "Especially in my data analytics projects, I have worked frequently with **Pandas** for tasks like **data cleaning**, **analysis**, and creating **summaries** from huge data sets.\n",
    "\n",
    "I mostly used **Numpy** to work with **arrays**, perform **fast numerical operations**, and enhance effectiveness in my code's **data-heavy parts**.\n",
    "\n",
    "A **GitHub-based data analytics dashboard** was one of my projects from pool of projects, where I examined **product data**, displayed **trends**, and developed a **Django REST API** to provide **insights**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc1e026",
   "metadata": {},
   "source": [
    "## **Conclusion**\n",
    "\n",
    "From the tests performed:\n",
    "\n",
    "- **Polars** generally offers faster execution times and better memory efficiency for large datasets.\n",
    "- **Pandas** is highly versatile and well-supported but can be slower on large data.\n",
    "- **Numpy** is excellent for pure numerical tasks but lacks the high-level data manipulation features.\n",
    "\n",
    "These insights can help in choosing the right library depending on project requirements.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
